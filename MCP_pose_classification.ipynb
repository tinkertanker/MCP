{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCP_pose_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6Ox4Cyr0LD6k"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MCP pose classification\n",
        "\n",
        "*TensorFlow pose classification from MoveNet data (adapted for 2022 Marina Central Project)*\n",
        "\n",
        "This notebook is adapted from the [TensorFlow pose classification tutorial part 2](https://www.tensorflow.org/lite/tutorials/pose_classification) to use the pre-processed and classified pose data we have to train a model to recognise the poses, and convert it to be used in a DepthAI OAK camera."
      ],
      "metadata": {
        "id": "6Ox4Cyr0LD6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "SIHuoBvZLvPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This installs the required packages."
      ],
      "metadata": {
        "id": "ChD0GPcULXdn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpvUMbzYLBHc",
        "outputId": "bc48afb4-a041-42ae-8df6-2e443162fb74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Requirement already satisfied: blobconverter in /usr/local/lib/python3.7/dist-packages (1.2.9)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from blobconverter) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from blobconverter) (2.27.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from blobconverter) (1.21.13)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->blobconverter) (0.5.2)\n",
            "Requirement already satisfied: botocore<1.25.0,>=1.24.13 in /usr/local/lib/python3.7/dist-packages (from boto3->blobconverter) (1.24.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->blobconverter) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.13->boto3->blobconverter) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.13->boto3->blobconverter) (1.26.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.13->boto3->blobconverter) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->blobconverter) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->blobconverter) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->blobconverter) (2.10)\n",
            "Requirement already satisfied: openvino-dev in /usr/local/lib/python3.7/dist-packages (2021.4.2)\n",
            "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (2.27.1)\n",
            "Requirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.8.1.post1)\n",
            "Requirement already satisfied: pandas~=1.1.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.1.5)\n",
            "Requirement already satisfied: editdistance>=0.5.3 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.5.3)\n",
            "Requirement already satisfied: rawpy>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.17.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.7.1)\n",
            "Requirement already satisfied: scipy~=1.5.4 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.5.4)\n",
            "Requirement already satisfied: tokenizers>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.11.6)\n",
            "Requirement already satisfied: nltk>=3.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (3.7)\n",
            "Requirement already satisfied: texttable~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.6.4)\n",
            "Requirement already satisfied: pillow>=8.1.2 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (9.0.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (6.0)\n",
            "Requirement already satisfied: parasail>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.2.4)\n",
            "Requirement already satisfied: py-cpuinfo>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (8.0.0)\n",
            "Requirement already satisfied: networkx~=2.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (2.6.3)\n",
            "Requirement already satisfied: fast-ctc-decode>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.3.0)\n",
            "Requirement already satisfied: jstyleson~=0.0.2 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.0.2)\n",
            "Requirement already satisfied: scikit-image>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.18.3)\n",
            "Requirement already satisfied: sentencepiece>=0.1.95 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.1.96)\n",
            "Requirement already satisfied: yamlloader>=0.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.1.0)\n",
            "Requirement already satisfied: openvino==2021.4.2 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (2021.4.2)\n",
            "Requirement already satisfied: addict>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (2.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.0.2)\n",
            "Requirement already satisfied: opencv-python==4.5.* in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (4.5.5.62)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (4.63.0)\n",
            "Requirement already satisfied: numpy<1.20,>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.19.5)\n",
            "Requirement already satisfied: nibabel>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (3.2.2)\n",
            "Requirement already satisfied: progress>=1.5 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (1.6)\n",
            "Requirement already satisfied: pydicom>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (2.2.2)\n",
            "Requirement already satisfied: hyperopt~=0.1.2 in /usr/local/lib/python3.7/dist-packages (from openvino-dev) (0.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt~=0.1.2->openvino-dev) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt~=0.1.2->openvino-dev) (1.15.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt~=0.1.2->openvino-dev) (4.0.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from nibabel>=3.2.1->openvino-dev) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nibabel>=3.2.1->openvino-dev) (57.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.5->openvino-dev) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.5->openvino-dev) (2022.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.5->openvino-dev) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->nibabel>=3.2.1->openvino-dev) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas~=1.1.5->openvino-dev) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas~=1.1.5->openvino-dev) (2018.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->openvino-dev) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->openvino-dev) (1.26.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->openvino-dev) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->openvino-dev) (2021.10.8)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.17.2->openvino-dev) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.17.2->openvino-dev) (1.2.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.17.2->openvino-dev) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.17.2->openvino-dev) (2.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.17.2->openvino-dev) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.17.2->openvino-dev) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->openvino-dev) (3.1.0)\n",
            "Cloning into 'examples'...\n",
            "remote: Enumerating objects: 20667, done.\u001b[K\n",
            "remote: Counting objects: 100% (2487/2487), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1084/1084), done.\u001b[K\n",
            "remote: Total 20667 (delta 1204), reused 2134 (delta 1043), pack-reused 18180\u001b[K\n",
            "Receiving objects: 100% (20667/20667), 34.15 MiB | 29.94 MiB/s, done.\n",
            "Resolving deltas: 100% (11298/11298), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!pip3 install -q opencv-python\n",
        "!pip3 install blobconverter\n",
        "!pip3 install openvino-dev\n",
        "!rm -rf examples\n",
        "!wget -q -O movenet_thunder.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
        "!git clone https://github.com/tensorflow/examples.git\n",
        "\n",
        "pose_sample_rpi_path = os.path.join(os.getcwd(), 'examples/lite/examples/pose_estimation/raspberry_pi')\n",
        "sys.path.append(pose_sample_rpi_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required packages"
      ],
      "metadata": {
        "id": "FcF7YW0mLh7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import cv2\n",
        "import itertools\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import tqdm\n",
        "import random\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import utils\n",
        "from data import BodyPart\n"
      ],
      "metadata": {
        "id": "MkDdoT3FLncK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess CSVs"
      ],
      "metadata": {
        "id": "LkqFDRhhL1pL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, parse CSVs outputted by MoveNet for training, and splits it into test/train CSVs.\n",
        "\n",
        "The .csv file structure:\n",
        "\n",
        "```\n",
        "filename,x1,y1,x2,y2...x17,y17 (17 landmarks)\n",
        "```\n",
        "\n",
        "Required file structure:\n",
        "\n",
        "```\n",
        "pose_data\n",
        "  pose_name_1.csv\n",
        "  pose_name_2.csv\n",
        "  ...\n",
        "```"
      ],
      "metadata": {
        "id": "T0IR8kthQkik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive and import files into runtime (optional; files can be manually uploaded into `/content/pose_data` as needed"
      ],
      "metadata": {
        "id": "yAZSZbk8OM3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!rm -rf /content/pose_data/\n",
        "!mkdir /content/pose_data/\n",
        "!cp -r /content/drive/MyDrive/pose_data/* /content/pose_data/ # replace drive folder path as needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1GmRoFhMHNE",
        "outputId": "8d865939-f7fa-42e0-9a99-4411978d4f47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consolidate CSVs. (Code adapted from [demo](https://github.com/geaxgx/depthai_movenet/blob/main/examples/yoga_pose_recognition/demo.py))"
      ],
      "metadata": {
        "id": "pox2DOqNQI1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "pose_samples_folder = '/content/pose_data/'\n",
        "file_extension = 'csv'\n",
        "file_delimiter = ','\n",
        "n_landmarks = 17\n",
        "n_dimensions = 2"
      ],
      "metadata": {
        "id": "zOSIPS0bVHLY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = []\n",
        "file_names = [name for name in os.listdir(pose_samples_folder) if name.endswith(file_extension)]\n",
        "pose_samples = []\n",
        "for i, file_name in enumerate(file_names):\n",
        "  # Use file name as pose class name\n",
        "  class_name = file_name[:-(len(file_extension) + 1)]\n",
        "  class_names.insert(i, class_name)\n",
        "  pose_samples.insert(i, [])\n",
        "  # Parse CSV\n",
        "  with open(os.path.join(pose_samples_folder, file_name)) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=file_delimiter)\n",
        "    for row in csv_reader:\n",
        "      assert len(row) == n_landmarks * n_dimensions + 1, 'Wrong number of values: {}'.format(len(row))\n",
        "      landmarks = np.array(row[1:], np.float32).reshape([n_landmarks, n_dimensions])\n",
        "      pose_sample = [ *row[1:], i, class_name]\n",
        "      pose_samples[i].append(pose_sample)"
      ],
      "metadata": {
        "id": "fTcsz6LqQteL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split test-train\n",
        "(adapted from pose classification tutorial)"
      ],
      "metadata": {
        "id": "DhHXh6HaXlyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_split = 0.15"
      ],
      "metadata": {
        "id": "dKtm9HrZXkf6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_poses = []\n",
        "test_poses = []\n",
        "\n",
        "for i in range(len(class_names)):\n",
        "  test_poses.insert(i, [])\n",
        "  train_poses.insert(i, [])\n",
        "\n",
        "  # Shuffle the data deterministically\n",
        "  pose_samples[i].sort()\n",
        "  random.seed(42)\n",
        "  random.shuffle(pose_samples[i])\n",
        "\n",
        "  test_count = int(len(pose_samples[i]) * test_split)\n",
        "\n",
        "  for j, sample in enumerate(pose_samples[i]):\n",
        "    if j < test_count:\n",
        "      test_poses[i].append(sample)\n",
        "    else:\n",
        "      train_poses[i].append(sample)"
      ],
      "metadata": {
        "id": "SfSAlkoMXu_s"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write test dataset and train dataset to respective CSVs."
      ],
      "metadata": {
        "id": "KTikNk2Wdt_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv_name = 'pose_train_data.csv'\n",
        "test_csv_name = 'pose_test_data.csv'\n",
        "\n",
        "landmark_names = [\n",
        "    'nose',\n",
        "    'left_eye', \n",
        "    'right_eye',\n",
        "    'left_ear', 'right_ear',\n",
        "    'left_shoulder', 'right_shoulder',\n",
        "    'left_elbow', 'right_elbow',\n",
        "    'left_wrist', 'right_wrist',\n",
        "    'left_hip', 'right_hip',\n",
        "    'left_knee', 'right_knee',\n",
        "    'left_ankle', 'right_ankle',\n",
        "]\n",
        "csv_headers = [\n",
        "    *itertools.chain.from_iterable([[name + '_x', name + '_y'] for name in landmark_names]),\n",
        "    'class_no', 'class_name'\n",
        "]"
      ],
      "metadata": {
        "id": "N9IUqnWCXzsX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_poses_rows = list(itertools.chain.from_iterable(train_poses))\n",
        "test_poses_rows = list(itertools.chain.from_iterable(test_poses))\n",
        "\n",
        "!cd /content/\n",
        "\n",
        "with open(train_csv_name, 'w', newline='') as train_csv:\n",
        "  writer = csv.writer(train_csv, delimiter=',')\n",
        "  writer.writerow(csv_headers)\n",
        "  for row in train_poses_rows:\n",
        "    writer.writerow(row)\n",
        "\n",
        "with open(test_csv_name, 'w', newline='') as test_csv:\n",
        "  writer = csv.writer(test_csv, delimiter=',')\n",
        "  writer.writerow(csv_headers)\n",
        "  for row in test_poses_rows:\n",
        "    writer.writerow(row)"
      ],
      "metadata": {
        "id": "UIaUSnyBhpsR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "2rdeKmeVlSEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "JQyXgGbFm3JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pose_landmarks(csv_path):\n",
        "  \"\"\"Loads a CSV created by MoveNetPreprocessor.\n",
        "  \n",
        "  Returns:\n",
        "    X: Detected landmark coordinates (N, 17 * 2)\n",
        "    y: Ground truth labels of shape (N, label_count)\n",
        "    classes: The list of all class names found in the dataset\n",
        "    dataframe: The CSV loaded as a Pandas dataframe features (X) and ground\n",
        "      truth labels (y) to use later to train a pose classification model.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load the CSV file\n",
        "  dataframe = pd.read_csv(csv_path)\n",
        "  df_to_process = dataframe.copy()\n",
        "\n",
        "  # Extract the list of class names\n",
        "  classes = df_to_process.pop('class_name').unique()\n",
        "\n",
        "  # Extract the labels\n",
        "  y = df_to_process.pop('class_no')\n",
        "\n",
        "  # Convert the input features and labels into the correct format for training.\n",
        "  X = df_to_process.astype('float64')\n",
        "  y = keras.utils.to_categorical(y)\n",
        "\n",
        "  return X, y, classes, dataframe"
      ],
      "metadata": {
        "id": "N3vF4RSOl8GT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the train data\n",
        "X, y, class_names, _ = load_pose_landmarks(train_csv_name)\n",
        "\n",
        "# Split training data (X, y) into (X_train, y_train) and (X_val, y_val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
        "                                                  test_size=0.15)"
      ],
      "metadata": {
        "id": "lrg9Ki3DnWOI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "X_test, y_test, _, df_test = load_pose_landmarks(test_csv_name)"
      ],
      "metadata": {
        "id": "9kT_lIFNnZ1u"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title FullBodyPoseEmbedder class\n",
        "#@markdown Taken from demo.py. Remember to run the block even though\n",
        "#@markdown the code is hidden.\n",
        "\n",
        "class FullBodyPoseEmbedder(object):\n",
        "  \"\"\"Converts pose landmarks into embedding.\"\"\"\n",
        "\n",
        "  def __init__(self, torso_size_multiplier=2.5):\n",
        "    # Multiplier to apply to the torso to get minimal body size.\n",
        "    self._torso_size_multiplier = torso_size_multiplier\n",
        "\n",
        "    # Names of the landmarks as they appear in the prediction.\n",
        "    self._landmark_names = [\n",
        "        'nose',\n",
        "        'left_eye', \n",
        "        'right_eye',\n",
        "        'left_ear', 'right_ear',\n",
        "        'left_shoulder', 'right_shoulder',\n",
        "        'left_elbow', 'right_elbow',\n",
        "        'left_wrist', 'right_wrist',\n",
        "        'left_hip', 'right_hip',\n",
        "        'left_knee', 'right_knee',\n",
        "        'left_ankle', 'right_ankle',\n",
        "    ]\n",
        "\n",
        "  def __call__(self, landmarks):\n",
        "    \"\"\"Normalizes pose landmarks and converts to embedding\n",
        "    \n",
        "    Args:\n",
        "      landmarks - NumPy array with 3D landmarks of shape (N, 3).\n",
        "    Result:\n",
        "      Numpy array with pose embedding of shape (M, 3) where `M` is the number of\n",
        "      pairwise distances defined in `_get_pose_distance_embedding`.\n",
        "    \"\"\"\n",
        "    print(landmarks.shape[0])\n",
        "    print(len(self._landmark_names))\n",
        "    # assert landmarks.shape[0] == len(self._landmark_names), 'Unexpected number of landmarks: {}'.format(landmarks.shape[0])\n",
        "\n",
        "    # Get pose landmarks.\n",
        "    landmarks = np.copy(landmarks)\n",
        "\n",
        "    # Normalize landmarks.\n",
        "    landmarks = self._normalize_pose_landmarks(landmarks)\n",
        "\n",
        "    # Get embedding. HERE\n",
        "    embedding = self._get_pose_distance_embedding(landmarks)\n",
        "\n",
        "    return embedding\n",
        "\n",
        "  def _normalize_pose_landmarks(self, landmarks):\n",
        "    \"\"\"Normalizes landmarks translation and scale.\"\"\"\n",
        "    landmarks = np.copy(landmarks)\n",
        "\n",
        "    # Normalize translation.\n",
        "    pose_center = self._get_pose_center(landmarks)\n",
        "    landmarks -= pose_center\n",
        "\n",
        "    # Normalize scale.\n",
        "    pose_size = self._get_pose_size(landmarks, self._torso_size_multiplier)\n",
        "    landmarks /= pose_size\n",
        "    # Multiplication by 100 is not required, but makes it eaasier to debug.\n",
        "    landmarks *= 100\n",
        "\n",
        "    return landmarks\n",
        "\n",
        "  def _get_pose_center(self, landmarks):\n",
        "    \"\"\"Calculates pose center as point between hips.\"\"\"\n",
        "    left_hip = landmarks[self._landmark_names.index('left_hip')]\n",
        "    right_hip = landmarks[self._landmark_names.index('right_hip')]\n",
        "    center = (left_hip + right_hip) * 0.5\n",
        "    return center\n",
        "\n",
        "  def _get_pose_size(self, landmarks, torso_size_multiplier):\n",
        "    \"\"\"Calculates pose size.\n",
        "    \n",
        "    It is the maximum of two values:\n",
        "      * Torso size multiplied by `torso_size_multiplier`\n",
        "      * Maximum distance from pose center to any pose landmark\n",
        "    \"\"\"\n",
        "    # This approach uses only 2D landmarks to compute pose size.\n",
        "    landmarks = landmarks[:, :2]\n",
        "\n",
        "    # Hips center.\n",
        "    left_hip = landmarks[self._landmark_names.index('left_hip')]\n",
        "    right_hip = landmarks[self._landmark_names.index('right_hip')]\n",
        "    hips = (left_hip + right_hip) * 0.5\n",
        "\n",
        "    # Shoulders center.\n",
        "    left_shoulder = landmarks[self._landmark_names.index('left_shoulder')]\n",
        "    right_shoulder = landmarks[self._landmark_names.index('right_shoulder')]\n",
        "    shoulders = (left_shoulder + right_shoulder) * 0.5\n",
        "\n",
        "    # Torso size as the minimum body size.\n",
        "    torso_size = np.linalg.norm(shoulders - hips)\n",
        "\n",
        "    # Max dist to pose center.\n",
        "    pose_center = self._get_pose_center(landmarks)\n",
        "    max_dist = np.max(np.linalg.norm(landmarks - pose_center, axis=1))\n",
        "\n",
        "    return max(torso_size * torso_size_multiplier, max_dist)\n",
        "\n",
        "  def _get_pose_distance_embedding(self, landmarks):\n",
        "    \"\"\"Converts pose landmarks into 3D embedding.\n",
        "    We use several pairwise 3D distances to form pose embedding. All distances\n",
        "    include X and Y components with sign. We differnt types of pairs to cover\n",
        "    different pose classes. Feel free to remove some or add new.\n",
        "    \n",
        "    Args:\n",
        "      landmarks - NumPy array with 3D landmarks of shape (N, 3).\n",
        "    Result:\n",
        "      Numpy array with pose embedding of shape (M, 3) where `M` is the number of\n",
        "      pairwise distances.\n",
        "    \"\"\"\n",
        "    embedding = np.array([\n",
        "        # One joint.\n",
        "\n",
        "        self._get_distance(\n",
        "            self._get_average_by_names(landmarks, 'left_hip', 'right_hip'),\n",
        "            self._get_average_by_names(landmarks, 'left_shoulder', 'right_shoulder')),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_elbow'),\n",
        "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_elbow'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_elbow', 'left_wrist'),\n",
        "        self._get_distance_by_names(landmarks, 'right_elbow', 'right_wrist'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_hip', 'left_knee'),\n",
        "        self._get_distance_by_names(landmarks, 'right_hip', 'right_knee'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_knee', 'left_ankle'),\n",
        "        self._get_distance_by_names(landmarks, 'right_knee', 'right_ankle'),\n",
        "\n",
        "        # Two joints.\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_wrist'),\n",
        "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_wrist'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_hip', 'left_ankle'),\n",
        "        self._get_distance_by_names(landmarks, 'right_hip', 'right_ankle'),\n",
        "\n",
        "        # Four joints.\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_hip', 'left_wrist'),\n",
        "        self._get_distance_by_names(landmarks, 'right_hip', 'right_wrist'),\n",
        "\n",
        "        # Five joints.\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_ankle'),\n",
        "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_ankle'),\n",
        "        \n",
        "        self._get_distance_by_names(landmarks, 'left_hip', 'left_wrist'),\n",
        "        self._get_distance_by_names(landmarks, 'right_hip', 'right_wrist'),\n",
        "\n",
        "        # Cross body.\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_elbow', 'right_elbow'),\n",
        "        self._get_distance_by_names(landmarks, 'left_knee', 'right_knee'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_wrist', 'right_wrist'),\n",
        "        self._get_distance_by_names(landmarks, 'left_ankle', 'right_ankle'),\n",
        "\n",
        "        # Body bent direction.\n",
        "\n",
        "        # self._get_distance(\n",
        "        #     self._get_average_by_names(landmarks, 'left_wrist', 'left_ankle'),\n",
        "        #     landmarks[self._landmark_names.index('left_hip')]),\n",
        "        # self._get_distance(\n",
        "        #     self._get_average_by_names(landmarks, 'right_wrist', 'right_ankle'),\n",
        "        #     landmarks[self._landmark_names.index('right_hip')]),\n",
        "    ])\n",
        "\n",
        "    return embedding\n",
        "\n",
        "  def _get_average_by_names(self, landmarks, name_from, name_to):\n",
        "    lmk_from = landmarks[self._landmark_names.index(name_from)]\n",
        "    lmk_to = landmarks[self._landmark_names.index(name_to)]\n",
        "    return (lmk_from + lmk_to) * 0.5\n",
        "\n",
        "  def _get_distance_by_names(self, landmarks, name_from, name_to):\n",
        "    lmk_from = landmarks[self._landmark_names.index(name_from)]\n",
        "    lmk_to = landmarks[self._landmark_names.index(name_to)]\n",
        "    return self._get_distance(lmk_from, lmk_to)\n",
        "\n",
        "  def _get_distance(self, lmk_from, lmk_to):\n",
        "    return lmk_to - lmk_from"
      ],
      "metadata": {
        "id": "I2RRzmStp7ZC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model"
      ],
      "metadata": {
        "id": "JCsXtRTQsBVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_center_point(landmarks, left_bodypart, right_bodypart):\n",
        "  \"\"\"Calculates the center point of the two given landmarks.\"\"\"\n",
        "\n",
        "  left = tf.gather(landmarks, left_bodypart.value, axis=1)\n",
        "  right = tf.gather(landmarks, right_bodypart.value, axis=1)\n",
        "  center = left * 0.5 + right * 0.5\n",
        "  return center\n",
        "\n",
        "\n",
        "def get_pose_size(landmarks, torso_size_multiplier=2.5):\n",
        "  \"\"\"Calculates pose size.\n",
        "\n",
        "  It is the maximum of two values:\n",
        "    * Torso size multiplied by `torso_size_multiplier`\n",
        "    * Maximum distance from pose center to any pose landmark\n",
        "  \"\"\"\n",
        "  # Hips center\n",
        "  hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
        "                                 BodyPart.RIGHT_HIP)\n",
        "\n",
        "  # Shoulders center\n",
        "  shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,\n",
        "                                      BodyPart.RIGHT_SHOULDER)\n",
        "\n",
        "  # Torso size as the minimum body size\n",
        "  torso_size = tf.linalg.norm(shoulders_center - hips_center)\n",
        "\n",
        "  # Pose center\n",
        "  pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
        "                                     BodyPart.RIGHT_HIP)\n",
        "  pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n",
        "  # Broadcast the pose center to the same size as the landmark vector to\n",
        "  # perform substraction\n",
        "  pose_center_new = tf.broadcast_to(pose_center_new,\n",
        "                                    [tf.size(landmarks) // (17*2), 17, 2])\n",
        "\n",
        "  # Dist to pose center\n",
        "  d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n",
        "                name=\"dist_to_pose_center\")\n",
        "  # Max dist to pose center\n",
        "  max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n",
        "\n",
        "  # Normalize scale\n",
        "  pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n",
        "\n",
        "  return pose_size\n",
        "\n",
        "\n",
        "def normalize_pose_landmarks(landmarks):\n",
        "  \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n",
        "  scaling it to a constant pose size.\n",
        "  \"\"\"\n",
        "  # Move landmarks so that the pose center becomes (0,0)\n",
        "  pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
        "                                 BodyPart.RIGHT_HIP)\n",
        "  pose_center = tf.expand_dims(pose_center, axis=1)\n",
        "  # Broadcast the pose center to the same size as the landmark vector to perform\n",
        "  # substraction\n",
        "  pose_center = tf.broadcast_to(pose_center, \n",
        "                                [tf.size(landmarks) // (17*2), 17, 2])\n",
        "  landmarks = landmarks - pose_center\n",
        "\n",
        "  # Scale the landmarks to a constant pose size\n",
        "  pose_size = get_pose_size(landmarks)\n",
        "  landmarks /= pose_size\n",
        "\n",
        "  return landmarks\n",
        "\n",
        "\n",
        "def landmarks_to_embedding(landmarks_input):\n",
        "  \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n",
        "  # Reshape the flat input into a matrix with shape=(17, 2)\n",
        "  reshaped_inputs = keras.layers.Reshape((17, 2))(landmarks_input)\n",
        "\n",
        "  # Normalize landmarks 2D\n",
        "  landmarks = normalize_pose_landmarks(reshaped_inputs[:, :])\n",
        "\n",
        "  # Flatten the normalized landmark coordinates into a vector\n",
        "  embedding = keras.layers.Flatten()(landmarks)\n",
        "\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "weQ-qf2Hvzra"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "inputs = tf.keras.Input(shape=(34))\n",
        "embedding = landmarks_to_embedding(inputs)\n",
        "\n",
        "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)\n",
        "layer = keras.layers.Dropout(0.5)(layer)\n",
        "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
        "layer = keras.layers.Dropout(0.5)(layer)\n",
        "outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwWGxVWhqWmC",
        "outputId": "ff225a72-d50d-4958-8ed0-3fdf61d4b48d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 34)]         0           []                               \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)            (None, 17, 2)        0           ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2 (Sl  (None, 17, 2)       0           ['reshape_2[0][0]']              \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " tf.compat.v1.gather_18 (TFOpLa  (None, 2)           0           ['tf.__operators__.getitem_2[0][0\n",
            " mbda)                                                           ]']                              \n",
            "                                                                                                  \n",
            " tf.compat.v1.gather_19 (TFOpLa  (None, 2)           0           ['tf.__operators__.getitem_2[0][0\n",
            " mbda)                                                           ]']                              \n",
            "                                                                                                  \n",
            " tf.math.multiply_18 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_18[0][0]'] \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_19 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_19[0][0]'] \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 2)           0           ['tf.math.multiply_18[0][0]',    \n",
            " mbda)                                                            'tf.math.multiply_19[0][0]']    \n",
            "                                                                                                  \n",
            " tf.compat.v1.size_4 (TFOpLambd  ()                  0           ['tf.__operators__.getitem_2[0][0\n",
            " a)                                                              ]']                              \n",
            "                                                                                                  \n",
            " tf.expand_dims_4 (TFOpLambda)  (None, 1, 2)         0           ['tf.__operators__.add_8[0][0]'] \n",
            "                                                                                                  \n",
            " tf.compat.v1.floor_div_4 (TFOp  ()                  0           ['tf.compat.v1.size_4[0][0]']    \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.broadcast_to_4 (TFOpLambda)  (None, 17, 2)       0           ['tf.expand_dims_4[0][0]',       \n",
            "                                                                  'tf.compat.v1.floor_div_4[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.math.subtract_6 (TFOpLambda  (None, 17, 2)       0           ['tf.__operators__.getitem_2[0][0\n",
            " )                                                               ]',                              \n",
            "                                                                  'tf.broadcast_to_4[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.gather_24 (TFOpLa  (None, 2)           0           ['tf.math.subtract_6[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.compat.v1.gather_25 (TFOpLa  (None, 2)           0           ['tf.math.subtract_6[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.math.multiply_24 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_24[0][0]'] \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_25 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_25[0][0]'] \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 2)           0           ['tf.math.multiply_24[0][0]',    \n",
            " ambda)                                                           'tf.math.multiply_25[0][0]']    \n",
            "                                                                                                  \n",
            " tf.compat.v1.size_5 (TFOpLambd  ()                  0           ['tf.math.subtract_6[0][0]']     \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.compat.v1.gather_22 (TFOpLa  (None, 2)           0           ['tf.math.subtract_6[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.compat.v1.gather_23 (TFOpLa  (None, 2)           0           ['tf.math.subtract_6[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.compat.v1.gather_20 (TFOpLa  (None, 2)           0           ['tf.math.subtract_6[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.compat.v1.gather_21 (TFOpLa  (None, 2)           0           ['tf.math.subtract_6[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.expand_dims_5 (TFOpLambda)  (None, 1, 2)         0           ['tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " tf.compat.v1.floor_div_5 (TFOp  ()                  0           ['tf.compat.v1.size_5[0][0]']    \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.math.multiply_22 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_22[0][0]'] \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_23 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_23[0][0]'] \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_20 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_20[0][0]'] \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_21 (TFOpLambd  (None, 2)           0           ['tf.compat.v1.gather_21[0][0]'] \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.broadcast_to_5 (TFOpLambda)  (None, 17, 2)       0           ['tf.expand_dims_5[0][0]',       \n",
            "                                                                  'tf.compat.v1.floor_div_5[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 2)           0           ['tf.math.multiply_22[0][0]',    \n",
            " ambda)                                                           'tf.math.multiply_23[0][0]']    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TFOpLa  (None, 2)           0           ['tf.math.multiply_20[0][0]',    \n",
            " mbda)                                                            'tf.math.multiply_21[0][0]']    \n",
            "                                                                                                  \n",
            " tf.math.subtract_8 (TFOpLambda  (None, 17, 2)       0           ['tf.math.subtract_6[0][0]',     \n",
            " )                                                                'tf.broadcast_to_5[0][0]']      \n",
            "                                                                                                  \n",
            " tf.math.subtract_7 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_10[0][0]',\n",
            " )                                                                'tf.__operators__.add_9[0][0]'] \n",
            "                                                                                                  \n",
            " tf.compat.v1.gather_26 (TFOpLa  (17, 2)             0           ['tf.math.subtract_8[0][0]']     \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.compat.v1.norm_4 (TFOpLambd  ()                  0           ['tf.math.subtract_7[0][0]']     \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.compat.v1.norm_5 (TFOpLambd  (2,)                0           ['tf.compat.v1.gather_26[0][0]'] \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_26 (TFOpLambd  ()                  0           ['tf.compat.v1.norm_4[0][0]']    \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.reduce_max_2 (TFOpLamb  ()                  0           ['tf.compat.v1.norm_5[0][0]']    \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.math.maximum_2 (TFOpLambda)  ()                  0           ['tf.math.multiply_26[0][0]',    \n",
            "                                                                  'tf.math.reduce_max_2[0][0]']   \n",
            "                                                                                                  \n",
            " tf.math.truediv_2 (TFOpLambda)  (None, 17, 2)       0           ['tf.math.subtract_6[0][0]',     \n",
            "                                                                  'tf.math.maximum_2[0][0]']      \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 34)           0           ['tf.math.truediv_2[0][0]']      \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128)          4480        ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 128)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 64)           8256        ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 64)           0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 8)            520         ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 13,256\n",
            "Trainable params: 13,256\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Add a checkpoint callback to store the checkpoint that has the highest\n",
        "# validation accuracy.\n",
        "checkpoint_path = \"weights.best.hdf5\"\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                             monitor='val_accuracy',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='max')\n",
        "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
        "                                              patience=20)\n",
        "\n",
        "# Start training\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=200,\n",
        "                    batch_size=16,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[checkpoint, earlystopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpyqZ1SJt3OT",
        "outputId": "4b766d5a-404e-4f3b-8b10-1fcbf1cb7c18"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "48/49 [============================>.] - ETA: 0s - loss: 1.9492 - accuracy: 0.3398\n",
            "Epoch 1: val_accuracy improved from -inf to 0.42647, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 1s 9ms/step - loss: 1.9489 - accuracy: 0.3403 - val_loss: 1.8217 - val_accuracy: 0.4265\n",
            "Epoch 2/200\n",
            "47/49 [===========================>..] - ETA: 0s - loss: 1.6280 - accuracy: 0.4694\n",
            "Epoch 2: val_accuracy improved from 0.42647 to 0.54412, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 1.6226 - accuracy: 0.4740 - val_loss: 1.4771 - val_accuracy: 0.5441\n",
            "Epoch 3/200\n",
            "45/49 [==========================>...] - ETA: 0s - loss: 1.3170 - accuracy: 0.5514\n",
            "Epoch 3: val_accuracy improved from 0.54412 to 0.83824, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 1.3020 - accuracy: 0.5597 - val_loss: 1.1490 - val_accuracy: 0.8382\n",
            "Epoch 4/200\n",
            "45/49 [==========================>...] - ETA: 0s - loss: 1.0562 - accuracy: 0.6681\n",
            "Epoch 4: val_accuracy improved from 0.83824 to 0.85294, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 1.0645 - accuracy: 0.6649 - val_loss: 0.8790 - val_accuracy: 0.8529\n",
            "Epoch 5/200\n",
            "45/49 [==========================>...] - ETA: 0s - loss: 0.8869 - accuracy: 0.7389\n",
            "Epoch 5: val_accuracy did not improve from 0.85294\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.8701 - accuracy: 0.7455 - val_loss: 0.6841 - val_accuracy: 0.8529\n",
            "Epoch 6/200\n",
            "46/49 [===========================>..] - ETA: 0s - loss: 0.6869 - accuracy: 0.8152\n",
            "Epoch 6: val_accuracy improved from 0.85294 to 0.87500, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.8117 - val_loss: 0.5355 - val_accuracy: 0.8750\n",
            "Epoch 7/200\n",
            "43/49 [=========================>....] - ETA: 0s - loss: 0.6056 - accuracy: 0.8285\n",
            "Epoch 7: val_accuracy improved from 0.87500 to 0.89706, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 5ms/step - loss: 0.6130 - accuracy: 0.8273 - val_loss: 0.4388 - val_accuracy: 0.8971\n",
            "Epoch 8/200\n",
            "41/49 [========================>.....] - ETA: 0s - loss: 0.5409 - accuracy: 0.8415\n",
            "Epoch 8: val_accuracy did not improve from 0.89706\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.5427 - accuracy: 0.8416 - val_loss: 0.3724 - val_accuracy: 0.8971\n",
            "Epoch 9/200\n",
            "41/49 [========================>.....] - ETA: 0s - loss: 0.4554 - accuracy: 0.8811\n",
            "Epoch 9: val_accuracy improved from 0.89706 to 0.91176, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.4434 - accuracy: 0.8844 - val_loss: 0.3189 - val_accuracy: 0.9118\n",
            "Epoch 10/200\n",
            "46/49 [===========================>..] - ETA: 0s - loss: 0.4113 - accuracy: 0.8818\n",
            "Epoch 10: val_accuracy improved from 0.91176 to 0.91912, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.4062 - accuracy: 0.8857 - val_loss: 0.2941 - val_accuracy: 0.9191\n",
            "Epoch 11/200\n",
            "24/49 [=============>................] - ETA: 0s - loss: 0.3735 - accuracy: 0.9036\n",
            "Epoch 11: val_accuracy improved from 0.91912 to 0.94118, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.3571 - accuracy: 0.9130 - val_loss: 0.2575 - val_accuracy: 0.9412\n",
            "Epoch 12/200\n",
            "49/49 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.9026\n",
            "Epoch 12: val_accuracy improved from 0.94118 to 0.94853, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.3464 - accuracy: 0.9026 - val_loss: 0.2445 - val_accuracy: 0.9485\n",
            "Epoch 13/200\n",
            "45/49 [==========================>...] - ETA: 0s - loss: 0.3340 - accuracy: 0.9028\n",
            "Epoch 13: val_accuracy did not improve from 0.94853\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.9052 - val_loss: 0.2144 - val_accuracy: 0.9412\n",
            "Epoch 14/200\n",
            "48/49 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.9167\n",
            "Epoch 14: val_accuracy improved from 0.94853 to 0.95588, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.3048 - accuracy: 0.9169 - val_loss: 0.2047 - val_accuracy: 0.9559\n",
            "Epoch 15/200\n",
            "47/49 [===========================>..] - ETA: 0s - loss: 0.2615 - accuracy: 0.9348\n",
            "Epoch 15: val_accuracy did not improve from 0.95588\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.2748 - accuracy: 0.9325 - val_loss: 0.1789 - val_accuracy: 0.9559\n",
            "Epoch 16/200\n",
            "45/49 [==========================>...] - ETA: 0s - loss: 0.2940 - accuracy: 0.9264\n",
            "Epoch 16: val_accuracy did not improve from 0.95588\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.9273 - val_loss: 0.1715 - val_accuracy: 0.9559\n",
            "Epoch 17/200\n",
            "45/49 [==========================>...] - ETA: 0s - loss: 0.2528 - accuracy: 0.9222\n",
            "Epoch 17: val_accuracy did not improve from 0.95588\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.2529 - accuracy: 0.9234 - val_loss: 0.1608 - val_accuracy: 0.9559\n",
            "Epoch 18/200\n",
            "45/49 [==========================>...] - ETA: 0s - loss: 0.2530 - accuracy: 0.9292\n",
            "Epoch 18: val_accuracy improved from 0.95588 to 0.96324, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.2602 - accuracy: 0.9247 - val_loss: 0.1467 - val_accuracy: 0.9632\n",
            "Epoch 19/200\n",
            "44/49 [=========================>....] - ETA: 0s - loss: 0.2223 - accuracy: 0.9361\n",
            "Epoch 19: val_accuracy did not improve from 0.96324\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.2188 - accuracy: 0.9377 - val_loss: 0.1486 - val_accuracy: 0.9559\n",
            "Epoch 20/200\n",
            "43/49 [=========================>....] - ETA: 0s - loss: 0.2317 - accuracy: 0.9477\n",
            "Epoch 20: val_accuracy did not improve from 0.96324\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.2287 - accuracy: 0.9468 - val_loss: 0.1403 - val_accuracy: 0.9559\n",
            "Epoch 21/200\n",
            "44/49 [=========================>....] - ETA: 0s - loss: 0.2128 - accuracy: 0.9474\n",
            "Epoch 21: val_accuracy did not improve from 0.96324\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.2075 - accuracy: 0.9481 - val_loss: 0.1386 - val_accuracy: 0.9559\n",
            "Epoch 22/200\n",
            "47/49 [===========================>..] - ETA: 0s - loss: 0.2049 - accuracy: 0.9428\n",
            "Epoch 22: val_accuracy did not improve from 0.96324\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.2062 - accuracy: 0.9429 - val_loss: 0.1204 - val_accuracy: 0.9559\n",
            "Epoch 23/200\n",
            "38/49 [======================>.......] - ETA: 0s - loss: 0.1682 - accuracy: 0.9572\n",
            "Epoch 23: val_accuracy did not improve from 0.96324\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.1785 - accuracy: 0.9558 - val_loss: 0.1217 - val_accuracy: 0.9632\n",
            "Epoch 24/200\n",
            "41/49 [========================>.....] - ETA: 0s - loss: 0.2151 - accuracy: 0.9405\n",
            "Epoch 24: val_accuracy did not improve from 0.96324\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.2004 - accuracy: 0.9468 - val_loss: 0.1147 - val_accuracy: 0.9632\n",
            "Epoch 25/200\n",
            "45/49 [==========================>...] - ETA: 0s - loss: 0.1999 - accuracy: 0.9472\n",
            "Epoch 25: val_accuracy did not improve from 0.96324\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.9481 - val_loss: 0.1266 - val_accuracy: 0.9559\n",
            "Epoch 26/200\n",
            "47/49 [===========================>..] - ETA: 0s - loss: 0.1789 - accuracy: 0.9481\n",
            "Epoch 26: val_accuracy did not improve from 0.96324\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.9455 - val_loss: 0.1121 - val_accuracy: 0.9632\n",
            "Epoch 27/200\n",
            "49/49 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9416\n",
            "Epoch 27: val_accuracy improved from 0.96324 to 0.97059, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9416 - val_loss: 0.1073 - val_accuracy: 0.9706\n",
            "Epoch 28/200\n",
            "25/49 [==============>...............] - ETA: 0s - loss: 0.1961 - accuracy: 0.9450\n",
            "Epoch 28: val_accuracy improved from 0.97059 to 0.97794, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.1843 - accuracy: 0.9494 - val_loss: 0.0930 - val_accuracy: 0.9779\n",
            "Epoch 29/200\n",
            "46/49 [===========================>..] - ETA: 0s - loss: 0.1552 - accuracy: 0.9674\n",
            "Epoch 29: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.9649 - val_loss: 0.0963 - val_accuracy: 0.9706\n",
            "Epoch 30/200\n",
            "46/49 [===========================>..] - ETA: 0s - loss: 0.1735 - accuracy: 0.9457\n",
            "Epoch 30: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9442 - val_loss: 0.0996 - val_accuracy: 0.9632\n",
            "Epoch 31/200\n",
            "27/49 [===============>..............] - ETA: 0s - loss: 0.1582 - accuracy: 0.9514\n",
            "Epoch 31: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1522 - accuracy: 0.9597 - val_loss: 0.0937 - val_accuracy: 0.9632\n",
            "Epoch 32/200\n",
            "48/49 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.9583\n",
            "Epoch 32: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.9584 - val_loss: 0.0950 - val_accuracy: 0.9706\n",
            "Epoch 33/200\n",
            "26/49 [==============>...............] - ETA: 0s - loss: 0.1673 - accuracy: 0.9567\n",
            "Epoch 33: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.9571 - val_loss: 0.0825 - val_accuracy: 0.9706\n",
            "Epoch 34/200\n",
            "25/49 [==============>...............] - ETA: 0s - loss: 0.1338 - accuracy: 0.9675\n",
            "Epoch 34: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1465 - accuracy: 0.9623 - val_loss: 0.0908 - val_accuracy: 0.9632\n",
            "Epoch 35/200\n",
            "45/49 [==========================>...] - ETA: 0s - loss: 0.1535 - accuracy: 0.9597\n",
            "Epoch 35: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1487 - accuracy: 0.9623 - val_loss: 0.0862 - val_accuracy: 0.9706\n",
            "Epoch 36/200\n",
            "26/49 [==============>...............] - ETA: 0s - loss: 0.1081 - accuracy: 0.9712\n",
            "Epoch 36: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1494 - accuracy: 0.9597 - val_loss: 0.0886 - val_accuracy: 0.9779\n",
            "Epoch 37/200\n",
            "46/49 [===========================>..] - ETA: 0s - loss: 0.1148 - accuracy: 0.9688\n",
            "Epoch 37: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9675 - val_loss: 0.0781 - val_accuracy: 0.9779\n",
            "Epoch 38/200\n",
            "49/49 [==============================] - ETA: 0s - loss: 0.1441 - accuracy: 0.9623\n",
            "Epoch 38: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1441 - accuracy: 0.9623 - val_loss: 0.0757 - val_accuracy: 0.9632\n",
            "Epoch 39/200\n",
            "47/49 [===========================>..] - ETA: 0s - loss: 0.1338 - accuracy: 0.9614\n",
            "Epoch 39: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1340 - accuracy: 0.9610 - val_loss: 0.0799 - val_accuracy: 0.9632\n",
            "Epoch 40/200\n",
            "48/49 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9661\n",
            "Epoch 40: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1168 - accuracy: 0.9662 - val_loss: 0.0791 - val_accuracy: 0.9706\n",
            "Epoch 41/200\n",
            "26/49 [==============>...............] - ETA: 0s - loss: 0.1111 - accuracy: 0.9639\n",
            "Epoch 41: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1053 - accuracy: 0.9649 - val_loss: 0.0755 - val_accuracy: 0.9779\n",
            "Epoch 42/200\n",
            "49/49 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9675\n",
            "Epoch 42: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1187 - accuracy: 0.9675 - val_loss: 0.0775 - val_accuracy: 0.9779\n",
            "Epoch 43/200\n",
            "43/49 [=========================>....] - ETA: 0s - loss: 0.1033 - accuracy: 0.9738\n",
            "Epoch 43: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0973 - accuracy: 0.9766 - val_loss: 0.0766 - val_accuracy: 0.9779\n",
            "Epoch 44/200\n",
            "47/49 [===========================>..] - ETA: 0s - loss: 0.1017 - accuracy: 0.9774\n",
            "Epoch 44: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1061 - accuracy: 0.9753 - val_loss: 0.0735 - val_accuracy: 0.9779\n",
            "Epoch 45/200\n",
            "25/49 [==============>...............] - ETA: 0s - loss: 0.1052 - accuracy: 0.9675\n",
            "Epoch 45: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1024 - accuracy: 0.9727 - val_loss: 0.0707 - val_accuracy: 0.9779\n",
            "Epoch 46/200\n",
            "27/49 [===============>..............] - ETA: 0s - loss: 0.1139 - accuracy: 0.9722\n",
            "Epoch 46: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1073 - accuracy: 0.9727 - val_loss: 0.0684 - val_accuracy: 0.9779\n",
            "Epoch 47/200\n",
            "48/49 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9779\n",
            "Epoch 47: val_accuracy did not improve from 0.97794\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0928 - accuracy: 0.9779 - val_loss: 0.0724 - val_accuracy: 0.9779\n",
            "Epoch 48/200\n",
            "45/49 [==========================>...] - ETA: 0s - loss: 0.1040 - accuracy: 0.9792\n",
            "Epoch 48: val_accuracy improved from 0.97794 to 0.98529, saving model to weights.best.hdf5\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.1005 - accuracy: 0.9805 - val_loss: 0.0672 - val_accuracy: 0.9853\n",
            "Epoch 49/200\n",
            "44/49 [=========================>....] - ETA: 0s - loss: 0.0974 - accuracy: 0.9759\n",
            "Epoch 49: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0913 - accuracy: 0.9779 - val_loss: 0.0744 - val_accuracy: 0.9779\n",
            "Epoch 50/200\n",
            "48/49 [============================>.] - ETA: 0s - loss: 0.1019 - accuracy: 0.9714\n",
            "Epoch 50: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.1016 - accuracy: 0.9714 - val_loss: 0.0636 - val_accuracy: 0.9853\n",
            "Epoch 51/200\n",
            "46/49 [===========================>..] - ETA: 0s - loss: 0.0935 - accuracy: 0.9715\n",
            "Epoch 51: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9701 - val_loss: 0.0628 - val_accuracy: 0.9779\n",
            "Epoch 52/200\n",
            "44/49 [=========================>....] - ETA: 0s - loss: 0.0839 - accuracy: 0.9744\n",
            "Epoch 52: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9753 - val_loss: 0.0660 - val_accuracy: 0.9779\n",
            "Epoch 53/200\n",
            "46/49 [===========================>..] - ETA: 0s - loss: 0.0746 - accuracy: 0.9810\n",
            "Epoch 53: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9818 - val_loss: 0.0726 - val_accuracy: 0.9779\n",
            "Epoch 54/200\n",
            "46/49 [===========================>..] - ETA: 0s - loss: 0.0868 - accuracy: 0.9810\n",
            "Epoch 54: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0866 - accuracy: 0.9792 - val_loss: 0.0602 - val_accuracy: 0.9853\n",
            "Epoch 55/200\n",
            "40/49 [=======================>......] - ETA: 0s - loss: 0.0946 - accuracy: 0.9750\n",
            "Epoch 55: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.0866 - accuracy: 0.9792 - val_loss: 0.0634 - val_accuracy: 0.9853\n",
            "Epoch 56/200\n",
            "46/49 [===========================>..] - ETA: 0s - loss: 0.0817 - accuracy: 0.9823\n",
            "Epoch 56: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0813 - accuracy: 0.9818 - val_loss: 0.0742 - val_accuracy: 0.9779\n",
            "Epoch 57/200\n",
            "45/49 [==========================>...] - ETA: 0s - loss: 0.0835 - accuracy: 0.9792\n",
            "Epoch 57: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.9805 - val_loss: 0.0843 - val_accuracy: 0.9779\n",
            "Epoch 58/200\n",
            "46/49 [===========================>..] - ETA: 0s - loss: 0.0718 - accuracy: 0.9796\n",
            "Epoch 58: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0720 - accuracy: 0.9792 - val_loss: 0.0686 - val_accuracy: 0.9779\n",
            "Epoch 59/200\n",
            "44/49 [=========================>....] - ETA: 0s - loss: 0.0675 - accuracy: 0.9844\n",
            "Epoch 59: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9844 - val_loss: 0.0765 - val_accuracy: 0.9779\n",
            "Epoch 60/200\n",
            "43/49 [=========================>....] - ETA: 0s - loss: 0.0790 - accuracy: 0.9797\n",
            "Epoch 60: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9779 - val_loss: 0.0586 - val_accuracy: 0.9853\n",
            "Epoch 61/200\n",
            "42/49 [========================>.....] - ETA: 0s - loss: 0.0709 - accuracy: 0.9807\n",
            "Epoch 61: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.9818 - val_loss: 0.0629 - val_accuracy: 0.9779\n",
            "Epoch 62/200\n",
            "41/49 [========================>.....] - ETA: 0s - loss: 0.0814 - accuracy: 0.9771\n",
            "Epoch 62: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.0810 - accuracy: 0.9792 - val_loss: 0.0669 - val_accuracy: 0.9779\n",
            "Epoch 63/200\n",
            "41/49 [========================>.....] - ETA: 0s - loss: 0.0837 - accuracy: 0.9771\n",
            "Epoch 63: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9792 - val_loss: 0.0636 - val_accuracy: 0.9779\n",
            "Epoch 64/200\n",
            "42/49 [========================>.....] - ETA: 0s - loss: 0.0767 - accuracy: 0.9762\n",
            "Epoch 64: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9766 - val_loss: 0.0628 - val_accuracy: 0.9779\n",
            "Epoch 65/200\n",
            "49/49 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9792\n",
            "Epoch 65: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.9792 - val_loss: 0.0576 - val_accuracy: 0.9779\n",
            "Epoch 66/200\n",
            "42/49 [========================>.....] - ETA: 0s - loss: 0.0753 - accuracy: 0.9747\n",
            "Epoch 66: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.9740 - val_loss: 0.0658 - val_accuracy: 0.9779\n",
            "Epoch 67/200\n",
            "37/49 [=====================>........] - ETA: 0s - loss: 0.0631 - accuracy: 0.9882\n",
            "Epoch 67: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.9896 - val_loss: 0.0592 - val_accuracy: 0.9853\n",
            "Epoch 68/200\n",
            "39/49 [======================>.......] - ETA: 0s - loss: 0.0606 - accuracy: 0.9808\n",
            "Epoch 68: val_accuracy did not improve from 0.98529\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.9831 - val_loss: 0.0530 - val_accuracy: 0.9853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the training history to see whether you're overfitting.\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['TRAIN', 'VAL'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Mjp826yCt5Wt",
        "outputId": "9afa79d8-38e2-4a82-8eea-aacb26bf7c30"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using the TEST dataset\n",
        "loss, accuracy = model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lVovH6Mt58u",
        "outputId": "3b89f413-ddef-438c-b596-cc1d22a8a358"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draw the confusion matrix to better understand the model performance"
      ],
      "metadata": {
        "id": "us9PJZK2t_hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "  \"\"\"Plots the confusion matrix.\"\"\"\n",
        "  if normalize:\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    print(\"Normalized confusion matrix\")\n",
        "  else:\n",
        "    print('Confusion matrix, without normalization')\n",
        "\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes, rotation=55)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "  fmt = '.2f' if normalize else 'd'\n",
        "  thresh = cm.max() / 2.\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, format(cm[i, j], fmt),\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.tight_layout()\n",
        "\n",
        "# Classify pose in the TEST dataset using the trained model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert the prediction result to class name\n",
        "y_pred_label = [class_names[i] for i in np.argmax(y_pred, axis=1)]\n",
        "y_true_label = [class_names[i] for i in np.argmax(y_test, axis=1)]\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "plot_confusion_matrix(cm,\n",
        "                      class_names,\n",
        "                      title ='Confusion Matrix of Pose Classification Model')\n",
        "\n",
        "# Print the classification report\n",
        "print('\\nClassification Report:\\n', classification_report(y_true_label,\n",
        "                                                          y_pred_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "sONb_6oOt_Oo",
        "outputId": "ef8c2f71-465e-4eb4-b7c2-ea5f0164912b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       cobra       1.00      1.00      1.00         9\n",
            " downwarddog       1.00      1.00      1.00         9\n",
            "     leftdab       0.97      1.00      0.98        30\n",
            "    mountain       1.00      0.50      0.67         6\n",
            "    rightdab       1.00      1.00      1.00        30\n",
            "       squat       1.00      1.00      1.00        30\n",
            "      surren       1.00      1.00      1.00        30\n",
            "   triangle1       0.87      1.00      0.93        13\n",
            "\n",
            "    accuracy                           0.98       157\n",
            "   macro avg       0.98      0.94      0.95       157\n",
            "weighted avg       0.98      0.98      0.98       157\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export"
      ],
      "metadata": {
        "id": "MKg7B9Ta3HYh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmSESMetj1sa"
      },
      "source": [
        "## Export a Trained Inference Graph\n",
        "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZJ_AF5MD3hZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6611b3-a7f4-4cff-e2a1-de8c8c6a06db"
      },
      "source": [
        "#clean output directory if necessary to start fresh:\n",
        "\n",
        "%cd /content/\n",
        "!rm -rf /content/model/ \n",
        "os.makedirs('/content/model/', exist_ok=True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHoP90pUyKSq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "de742988-b27e-47e4-ccbd-5302816105e5"
      },
      "source": [
        "output_directory = '/content/model/'\n",
        "model_dir = '/content/model/saved_model/'\n",
        "\n",
        "# Save the entire model as a SavedModel.\n",
        "model.save(model_dir)\n",
        "\n",
        "# freeze model\n",
        "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "full_model = tf.function(lambda inputs: model(inputs))    \n",
        "full_model = full_model.get_concrete_function([tf.TensorSpec(model_input.shape, model_input.dtype) for model_input in model.inputs]) # Get frozen ConcreteFunction    \n",
        "frozen_func = convert_variables_to_constants_v2(full_model)\n",
        "frozen_func.graph.as_graph_def() # Save frozen graph from frozen ConcreteFunction to hard drive\n",
        "tf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=output_directory+\"frozen/\", name=\"frozen_inference_graph.pb\", as_text=False)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/model/saved_model/assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/model/frozen/frozen_inference_graph.pb'"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qucQJwsWhL3"
      },
      "source": [
        "# Prepare the TF model for DepthAI. \n",
        "# First, we convert the model to OpenVINO Intermediate Representation (IR)\n",
        "(This can be used to run inference on OpenVINO, but for our purpose it is only a step in the preparation for DepthAI)\n",
        "# In order to run the model on DepthAI modules, we then compile the IR obtained above to a .blob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7sqTX79W6hh"
      },
      "source": [
        "## Convert TF model to OpenVINO IR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsWggE5AIWS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "136c5ccf-da6b-4236-dea0-9911cee62f82"
      },
      "source": [
        "#CONVERT TF MODEL to OPEN VINO IRv10\n",
        "%cd \"/content/model/frozen\"\n",
        "!mo \\\n",
        "    --input_model frozen_inference_graph.pb \\\n",
        "    --output_dir signs \\\n",
        "    --data_type FP16 \\\n",
        "    --input_shape [34,17,2]\n",
        "    "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/model/frozen\n",
            "Model Optimizer arguments:\n",
            "Common parameters:\n",
            "\t- Path to the Input Model: \t/content/model/frozen/frozen_inference_graph.pb\n",
            "\t- Path for generated IR: \t/content/model/frozen/signs\n",
            "\t- IR output name: \tfrozen_inference_graph\n",
            "\t- Log level: \tERROR\n",
            "\t- Batch: \tNot specified, inherited from the model\n",
            "\t- Input layers: \tNot specified, inherited from the model\n",
            "\t- Output layers: \tNot specified, inherited from the model\n",
            "\t- Input shapes: \t[34,17,2]\n",
            "\t- Mean values: \tNot specified\n",
            "\t- Scale values: \tNot specified\n",
            "\t- Scale factor: \tNot specified\n",
            "\t- Precision of IR: \tFP16\n",
            "\t- Enable fusing: \tTrue\n",
            "\t- Enable grouped convolutions fusing: \tTrue\n",
            "\t- Move mean values to preprocess section: \tNone\n",
            "\t- Reverse input channels: \tFalse\n",
            "TensorFlow specific parameters:\n",
            "\t- Input model in text protobuf format: \tFalse\n",
            "\t- Path to model dump for TensorBoard: \tNone\n",
            "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
            "\t- Update the configuration file with input/output node names: \tNone\n",
            "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
            "\t- Use the config file: \tNone\n",
            "\t- Inference Engine found in: \t/usr/local/lib/python3.7/dist-packages/openvino\n",
            "Inference Engine version: \t2021.4.2-3976-0943ed67223-refs/pull/539/head\n",
            "Model Optimizer version: \t2021.4.2-3976-0943ed67223-refs/pull/539/head\n",
            "/usr/local/lib/python3.7/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "  import imp\n",
            "[ SUCCESS ] Generated IR version 10 model.\n",
            "[ SUCCESS ] XML file: /content/model/frozen/signs/frozen_inference_graph.xml\n",
            "[ SUCCESS ] BIN file: /content/model/frozen/signs/frozen_inference_graph.bin\n",
            "[ SUCCESS ] Total execution time: 16.21 seconds. \n",
            "[ SUCCESS ] Memory consumed: 487 MB. \n",
            "It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzArRZOALo_6"
      },
      "source": [
        "## Compile the IR model to a .blob for use on DepthAI modules/platform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUYtu18IMFoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f4ec30-7d1b-493c-a4dc-784790b59060"
      },
      "source": [
        "import os\n",
        "os.environ['IR_XML'] = \"/content/model/frozen/signs/frozen_inference_graph.xml\"\n",
        "os.environ['IR_BIN'] = \"/content/model/frozen/signs/frozen_inference_graph.bin\"\n",
        "os.environ['BLOB_DIR'] = \"/content/model/saved_model_blob/\"\n",
        "\n",
        "!rm -rf $BLOB_DIR && mkdir $BLOB_DIR && cd $BLOB_DIR\n",
        "!blobconverter \\\n",
        "      -o $BLOB_DIR \\\n",
        "      -ox $IR_XML \\\n",
        "      -ob $IR_BIN \\\n",
        "      --shaves 7"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /content/model/saved_model_blob/frozen_inference_graph_openvino_2021.4_7shave.blob...\n",
            "[==================================================]\n",
            "Done\n",
            "/content/model/saved_model_blob/frozen_inference_graph_openvino_2021.4_7shave.blob\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjcjk9TGMOxq"
      },
      "source": [
        "##Now you can download your .blob file and run it on the DepthAI module/platform\n",
        "To download locally, use the file explorer on the left to locate the file in the `$BLOB_DIR` folder, then right click download. Colab takes a few seconds to prepare the file, then the download prompt will appear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVo0ZZF3f1dn"
      },
      "source": [
        "## To run the .blob in DepthAI, follow the tutorial:\n",
        "https://docs.luxonis.com/en/latest/pages/tutorials/hello_world/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YH94kNkM5Uk"
      },
      "source": [
        "[Optional] to convert the blob locally, download the IR files .bin and .xml and follow these instructions:\n",
        "\n",
        "https://docs.luxonis.com/en/latest/pages/tutorials/local_convert_openvino/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOBXTQGqMgH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e70f068-7bec-459f-df83-8999294f7af6"
      },
      "source": [
        "#compress the IR_V10 folder\n",
        "%cd /content/\n",
        "!tar cvf model.tar.gz model\n"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "model/\n",
            "model/saved_model/\n",
            "model/saved_model/assets/\n",
            "model/saved_model/variables/\n",
            "model/saved_model/variables/variables.index\n",
            "model/saved_model/variables/variables.data-00000-of-00001\n",
            "model/saved_model/keras_metadata.pb\n",
            "model/saved_model/saved_model.pb\n",
            "model/saved_model_blob/\n",
            "model/saved_model_blob/frozen_inference_graph_openvino_2021.4_7shave.blob\n",
            "model/frozen/\n",
            "model/frozen/movenet_thunder.tflite\n",
            "model/frozen/frozen_inference_graph.pb\n",
            "model/frozen/signs/\n",
            "model/frozen/signs/frozen_inference_graph.bin\n",
            "model/frozen/signs/frozen_inference_graph.xml\n",
            "model/frozen/signs/frozen_inference_graph.mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLzHC-E9Mo6v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1482cb80-b482-4ccf-bb18-3eaef06d28ed"
      },
      "source": [
        "#download the compressed IRv10 folder locally\n",
        "#or can use file navigator on the left to move it to your gdrive\n",
        "from google.colab import files\n",
        "files.download(\"model.tar.gz\")"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_df7351cd-4755-45d4-81bb-af0ad563d80d\", \"model.tar.gz\", 13322240)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}